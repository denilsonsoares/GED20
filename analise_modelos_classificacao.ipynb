{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aecf744b",
   "metadata": {},
   "source": [
    "### Comparação entre os modelos de classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80e46cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Avaliando: Random Forest -----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.53      0.59      2406\n",
      "           1       0.74      0.82      0.78      3901\n",
      "\n",
      "    accuracy                           0.71      6307\n",
      "   macro avg       0.69      0.68      0.68      6307\n",
      "weighted avg       0.71      0.71      0.71      6307\n",
      "\n",
      "Matriz de Confusão:\n",
      "[[1285 1121]\n",
      " [ 698 3203]]\n",
      "------------------------------\n",
      "\n",
      "----- Avaliando: XGBoost -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Denil\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\xgboost\\training.py:183: UserWarning: [12:05:21] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.57      0.60      2406\n",
      "           1       0.75      0.79      0.77      3901\n",
      "\n",
      "    accuracy                           0.70      6307\n",
      "   macro avg       0.69      0.68      0.68      6307\n",
      "weighted avg       0.70      0.70      0.70      6307\n",
      "\n",
      "Matriz de Confusão:\n",
      "[[1376 1030]\n",
      " [ 831 3070]]\n",
      "------------------------------\n",
      "\n",
      "----- Avaliando: Logistic Regression -----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.19      0.29      2406\n",
      "           1       0.65      0.90      0.75      3901\n",
      "\n",
      "    accuracy                           0.63      6307\n",
      "   macro avg       0.60      0.55      0.52      6307\n",
      "weighted avg       0.61      0.63      0.58      6307\n",
      "\n",
      "Matriz de Confusão:\n",
      "[[ 467 1939]\n",
      " [ 376 3525]]\n",
      "------------------------------\n",
      "\n",
      "----- Avaliando: SVM -----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.22      0.32      2406\n",
      "           1       0.65      0.91      0.76      3901\n",
      "\n",
      "    accuracy                           0.65      6307\n",
      "   macro avg       0.63      0.57      0.54      6307\n",
      "weighted avg       0.64      0.65      0.59      6307\n",
      "\n",
      "Matriz de Confusão:\n",
      "[[ 535 1871]\n",
      " [ 352 3549]]\n",
      "------------------------------\n",
      "\n",
      "===== Tabela Comparativa de Desempenho dos Modelos =====\n",
      "                Modelo  Acurácia  Precisão    Recall  F1-Score   ROC AUC\n",
      "0        Random Forest  0.711590  0.740749  0.821072  0.778845  0.775459\n",
      "1              XGBoost  0.704931  0.748780  0.786978  0.767404  0.773426\n",
      "3                  SVM  0.647534  0.654797  0.909767  0.761506  0.650145\n",
      "2  Logistic Regression  0.632948  0.645132  0.903614  0.752803  0.630601\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, roc_auc_score, confusion_matrix,\n",
    "                             classification_report)\n",
    "\n",
    "# --- 1. Carregamento e Alinhamento Inicial dos Dados ---\n",
    "try:\n",
    "    df_full = pd.read_csv('df_com_outliers.csv')\n",
    "    df_clean = pd.read_csv('df_sem_outliers.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Arquivos 'df_com_outliers.csv' e/ou 'df_sem_outliers.csv' não encontrados.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Encontrar Intersecção e Alinhar DataFrames ---\n",
    "# Encontrar os barcodes comuns a ambos os dataframes\n",
    "common_barcodes = np.intersect1d(df_full['barcode_hashed'], df_clean['barcode_hashed'])\n",
    "\n",
    "# Filtrar ambos os dataframes para conter apenas os barcodes comuns\n",
    "df_full_aligned = df_full[df_full['barcode_hashed'].isin(common_barcodes)].copy()\n",
    "df_clean_aligned = df_clean[df_clean['barcode_hashed'].isin(common_barcodes)].copy()\n",
    "\n",
    "# --- 3. Divisão Única dos Dados (baseada nos dados alinhados) ---\n",
    "# Usaremos o df_clean_aligned para a divisão, pois representa o conjunto de dados válidos após todo o tratamento\n",
    "X = df_clean_aligned.drop('has_failure', axis=1)\n",
    "y = df_clean_aligned['has_failure']\n",
    "\n",
    "# A divisão agora é feita em um X que AINDA CONTÉM o barcode_hashed\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# --- 4. Preparar os Conjuntos de Dados para Cada Grupo usando o barcode_hashed ---\n",
    "\n",
    "# Grupo 1: Modelos de Árvore (usam dados com outliers)\n",
    "# Usamos o merge para garantir o alinhamento perfeito pelas chaves de treino e teste\n",
    "X_train_tree = pd.merge(X_train[['barcode_hashed']], df_full_aligned, on='barcode_hashed', how='inner').drop('has_failure', axis=1)\n",
    "X_test_tree = pd.merge(X_test[['barcode_hashed']], df_full_aligned, on='barcode_hashed', how='inner').drop('has_failure', axis=1)\n",
    "\n",
    "# Grupo 2: Modelos Lineares/SVM (usam os dados já divididos do conjunto limpo)\n",
    "X_train_linear = X_train\n",
    "X_test_linear = X_test\n",
    "\n",
    "# --- 5. Remover a Coluna de ID e Aplicar Standardização ---\n",
    "\n",
    "# Agora removemos o barcode_hashed dos 4 dataframes de features\n",
    "id_col = 'barcode_hashed'\n",
    "X_train_tree_final = X_train_tree.drop(id_col, axis=1)\n",
    "X_test_tree_final = X_test_tree.drop(id_col, axis=1)\n",
    "X_train_linear_final = X_train_linear.drop(id_col, axis=1)\n",
    "X_test_linear_final = X_test_linear.drop(id_col, axis=1)\n",
    "\n",
    "# Standardização (apenas para o grupo 2)\n",
    "scaler = StandardScaler()\n",
    "X_train_linear_scaled = scaler.fit_transform(X_train_linear_final)\n",
    "X_test_linear_scaled = scaler.transform(X_test_linear_final)\n",
    "\n",
    "# --- 6. Treinamento e Avaliação Unificados ---\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    'XGBoost': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'SVM': SVC(random_state=42, probability=True)\n",
    "}\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n----- Avaliando: {name} -----\")\n",
    "    \n",
    "    # y_train e y_test são os mesmos para todos\n",
    "    if name in ['Random Forest', 'XGBoost']:\n",
    "        model.fit(X_train_tree_final, y_train)\n",
    "        y_pred = model.predict(X_test_tree_final)\n",
    "        y_pred_proba = model.predict_proba(X_test_tree_final)[:, 1]\n",
    "    else:\n",
    "        model.fit(X_train_linear_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_linear_scaled)\n",
    "        y_pred_proba = model.predict_proba(X_test_linear_scaled)[:, 1]\n",
    "\n",
    "    # Calcular e armazenar métricas\n",
    "    results_list.append({\n",
    "        'Modelo': name,\n",
    "        'Acurácia': accuracy_score(y_test, y_pred),\n",
    "        'Precisão': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1-Score': f1_score(y_test, y_pred),\n",
    "        'ROC AUC': roc_auc_score(y_test, y_pred_proba)\n",
    "    })\n",
    "    \n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Matriz de Confusão:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# --- 7. Comparação Final ---\n",
    "results_df = pd.DataFrame(results_list)\n",
    "print(\"\\n===== Tabela Comparativa de Desempenho dos Modelos =====\")\n",
    "print(results_df.sort_values(by='F1-Score', ascending=False).to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
